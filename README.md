# TensorFlow 2.0 GPT

Structure is as same as [imcaspar/gpt2-ml](https://github.com/imcaspar/gpt2-ml), not official work, because I want to read the pretrained Chinese model. ([Chinese pretrained model](https://drive.google.com/file/d/1mT_qCQg4AWnAXTwKfsyyRWCRpgPrBJS3))

Still some difference between the three GPT code which refer, check the call function of `Transformer` class in the `model.py` for more detail.

Paper:

- [https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf)


Other reference repo:

- Official repo (with TensorFlow 1.x) [openai/gpt-2](https://github.com/openai/gpt-2)
- [imcaspar/gpt2-ml](https://github.com/imcaspar/gpt2-ml)
- [karpathy/minGPT](https://github.com/karpathy/minGPT)
